{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85352f4c",
   "metadata": {},
   "source": [
    "Prepare data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d213efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51489fe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nursingHomeData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9588\\1254226486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnursingHomeDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nursingHomeData.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'FederalProviderNumber'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#nursingHomeDF.drop((nursingHomeDF.loc[(nursingHomeDF[\"InfectionScore\"] == \"Not Available\") & (nursingHomeDF[\"FacilityReadmissionScore\"] == \"Not Available\")]).index, inplace = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnursingHomeDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnursingHomeDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnursingHomeDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FacilityReadmissionScore\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Not Available\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnursingHomeDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nursingHomeData.csv'"
     ]
    }
   ],
   "source": [
    "nursingHomeDF = pd.read_csv(\"nursingHomeData.csv\", index_col = 'FederalProviderNumber')\n",
    "#nursingHomeDF.drop((nursingHomeDF.loc[(nursingHomeDF[\"InfectionScore\"] == \"Not Available\") & (nursingHomeDF[\"FacilityReadmissionScore\"] == \"Not Available\")]).index, inplace = True)\n",
    "nursingHomeDF.drop((nursingHomeDF.loc[(nursingHomeDF[\"FacilityReadmissionScore\"] == \"Not Available\")]).index, inplace = True)\n",
    "\n",
    "trainDF, testDF = train_test_split(nursingHomeDF, test_size = 0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.drop((trainDF.loc[(trainDF[\"InfectionScore\"] == \"Not Available\") & (trainDF[\"FacilityReadmissionScore\"] == \"Not Available\")]).index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c1417",
   "metadata": {},
   "source": [
    "First off we have too many columns to be easily human parsable, but as most of them are from one dataset we can ignore them for now and work on those few columns from other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc274fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "measureScoreColumns = trainDF.filter(like='Q').columns\n",
    "noMeasureScoreColumns = list(set(trainDF.columns) - set(measureScoreColumns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a521f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF[noMeasureScoreColumns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97233e",
   "metadata": {},
   "source": [
    "We will replace all NaN values in count columns with 0, under the assumption that if there was a penalty, it would have been reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsColumns = [\"fineCounts\", \"paymentDenialCounts\", \"StandardDeficiency\", \"ComplaintDeficiency\", \"InfectionControlInspectionDeficiency\", \"CitationunderIDR\", \"CitationunderIIDR\"]\n",
    "trainDF[countsColumns] = trainDF[countsColumns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF[noMeasureScoreColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fe539",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ee349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDF[noMeasureScoreColumns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c39077",
   "metadata": {},
   "outputs": [],
   "source": [
    "noNA = (trainDF.isnull().sum() == 0).tolist()\n",
    "hasNA = np.logical_not(noNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc155f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.loc[:,noNA].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.loc[:,hasNA].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fdefb",
   "metadata": {},
   "source": [
    "As the rest of the values are scores from an evaluation, there are multiple imputation options, the most obvious including mean or median imputation. In this case both options seem reasonable, but lets just go with mean imputation so that the mean of our scores doesn't change from imputation. The initial distributions can be seen below.\n",
    "\n",
    "As scaleing features to a constant variance needs to be done before PCA (we have alot of features and likely can get rid of a few) and as KNN is a distance based algorithm it seems reasonable to perform some form of scaling before using KNN to impute, so first we will scale using a standard scaler to set variance to 1 and mean to 0.\n",
    "While min/max scaling is ideal for KNN, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16661eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale features using standard scaler\n",
    "numerics = trainDF.select_dtypes(include='float64').columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(trainDF[numerics])\n",
    "trainDF[numerics] = scaler.transform(trainDF[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting histograms for our data\n",
    "trainDF.loc[:,hasNA].hist(figsize = (50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "meanImputedData = trainDF.copy()\n",
    "\n",
    "imputer.fit(meanImputedData.loc[:,hasNA])\n",
    "meanImputedData.loc[:,hasNA] = imputer.transform(meanImputedData.loc[:,hasNA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanImputedData.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947d469",
   "metadata": {},
   "source": [
    "And the resulting distributions after imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resulting histograms\n",
    "meanImputedData.loc[:,hasNA].hist(figsize = (50,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e0d7e",
   "metadata": {},
   "source": [
    "While most of the distributions look pretty similar, some have changed quite considerably.\n",
    "Lets see if a more complicated imputation scheme can perform better, in this case scikit-learns KNN imputer\n",
    "\n",
    ".hist(figsize = (50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNimp = KNNImputer(n_neighbors=2)\n",
    "KNNImputedData = trainDF.copy()\n",
    "\n",
    "KNNimp.fit(KNNImputedData.loc[:,hasNA])\n",
    "KNNImputedData.loc[:,hasNA] = KNNimp.transform(KNNImputedData.loc[:,hasNA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNImputedData.loc[:,hasNA].hist(figsize = (50,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f30cc",
   "metadata": {},
   "source": [
    "KNN imputed data looks a little more consistent with the initial data\n",
    "\n",
    "While were at it we can take a look at the rest of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32267866",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNImputedData.loc[:,noNA].hist(figsize = (50,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf347455",
   "metadata": {},
   "source": [
    "we have a lot of matricies, we can see that a lot of the distributions look similar, and as such may be linearly correlated. We can perform PCA to know which features are likely to add little information to our model. First check if our matrix is sparce, as scikit-learn's PCA requires a non-sparce matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0dd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = KNNImputedData\n",
    "sparse.issparse(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1eddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = trainDF.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "sn.heatmap(corrMatrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6fd38",
   "metadata": {},
   "source": [
    "From this we can see that there are mostly 4x4 blocks of correlation (the lighter the color, the higher the correlation), that is that the value of scores over 4 quarters are sometimes correlated, but that is not always the case.  \n",
    "Having as much uncorrelated data as we have is a good sign that there is at least a lot of information in our data, although it does not necessarily explain what we want to model.\n",
    "We can at least use PCA to remove a few correlated fields, although there will likely be quite a few leftover.\n",
    "\n",
    "If we recall our initial data preparation we have 18 measure codes which we have 4 quarters of data for, giving us 72 features, and 4 measure codes which have adjusted and expected scores, so 8 more features there, plus 5 counts of deficiencies, 2 other count features, and finally our two target features adding up to 87 trainable features and 2 labels.\n",
    "Below method based on: https://gist.github.com/rpromoditha/f73265e5a8db7084b521d79b2ecc3ece#file-pca_qas_2-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "pca.fit(trainDF[numerics])\n",
    "\n",
    "exp_var = pca.explained_variance_ratio_ * 100\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "plt.bar(range(87), exp_var, align='center',\n",
    "        label='Individual explained variance')\n",
    "\n",
    "plt.step(range(87), cum_exp_var, where='mid',\n",
    "         label='Cumulative explained variance', color='red')\n",
    "\n",
    "plt.ylabel('Explained variance percentage')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0799291",
   "metadata": {},
   "source": [
    "From this we can see that we have very little correlation between our features, as show in the above heatmap. However, as we have 87 features, each feature inhenrently adds little to the explentation. As we can see above we can remove a large number of features with small additions by specifying that we would like to keep the best features that have a cummulative 80% explenation of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% variance - 29 features\n",
    "pca = PCA(n_components=0.80)\n",
    "pca.fit(trainDF[numerics])\n",
    "trainX = pca.transform(trainDF[numerics])\n",
    "print(trainX.shape)\n",
    "trainY = trainDF[['InfectionScore', 'FacilityReadmissionScore']]\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655ce27",
   "metadata": {},
   "source": [
    "Our data is now prepared enough for modeling, we can make the above into a function and ensure we prepare our training and testing data in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns train/test data split into features and labels in format: trainX, trainY, testX, testY\n",
    "#target can be FacilityReadmissionScore or InfectionScore\n",
    "#copied into a python file so it can be imported\n",
    "#this is the initial process, updates may be made in the dataPrep.py file that are not from this process\n",
    "\n",
    "def preprocessData(data, splitSeed = 0, target = 'FacilityReadmissionScore'):\n",
    "    #import statements\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy import sparse\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    #drop rows without a target\n",
    "    data.drop((data.loc[(data[target] == \"Not Available\")]).index, inplace = True)\n",
    "    \n",
    "    #split data into train and test\n",
    "    trainDF, testDF = train_test_split(data, test_size = 0.3, random_state = splitSeed)\n",
    "    \n",
    "    #the count based column names saved to a list\n",
    "    countsColumns = [\"fineCounts\", \"paymentDenialCounts\", \"StandardDeficiency\", \"ComplaintDeficiency\", \"InfectionControlInspectionDeficiency\", \"CitationunderIDR\", \"CitationunderIIDR\"]\n",
    "    \n",
    "    #replace count based nulls with zeroes\n",
    "    trainDF[countsColumns] = trainDF[countsColumns].fillna(0)\n",
    "    testDF[countsColumns] = testDF[countsColumns].fillna(0)\n",
    "    \n",
    "    #as we are using KNN imputing, we should scale numeric values between 0 and 1 first so distance measures are consistant\n",
    "    numerics = trainDF.select_dtypes(include='float64').columns\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trainDF[numerics])\n",
    "    trainDF[numerics] = scaler.transform(trainDF[numerics])\n",
    "    testDF[numerics] = scaler.transform(testDF[numerics])\n",
    "    \n",
    "    \n",
    "    #impute null values with KNN imputer\n",
    "    noNA = (trainDF.isnull().sum() == 0).tolist()\n",
    "    hasNA = np.logical_not(noNA)\n",
    "    \n",
    "    KNNimp = KNNImputer(n_neighbors=2)\n",
    "    KNNimp.fit(trainDF.loc[:,hasNA])\n",
    "    trainDF.loc[:,hasNA] = KNNimp.transform(trainDF.loc[:,hasNA])\n",
    "    testDF.loc[:,hasNA] = KNNimp.transform(testDF.loc[:,hasNA])\n",
    "    \n",
    "    #Perform PCA, separate train and test data\n",
    "    pca = PCA(n_components=0.80)\n",
    "    pca.fit(trainDF[numerics])\n",
    "    trainX = pca.transform(trainDF[numerics])\n",
    "    testX = pca.transform(testDF[numerics])\n",
    "    trainY = trainDF[target]\n",
    "    testY = testDF[target]\n",
    "        \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_fromFunction, trainY_fromFunction, testX_fromFunction, testY_fromFunction = preprocessData(nursingHomeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(trainX_fromFunction, trainX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
